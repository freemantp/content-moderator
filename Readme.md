## TODO
* use hw (gpu/tpu) support for inference
* use wsgi
* Dockerize
* Specify download location for models
* Use FastAPI
* Write tests
* Count requests